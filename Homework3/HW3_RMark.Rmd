---
title: "HW3"
author: "Hannah Zmuda"
date: "11/18/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library,include=FALSE}
library(ggplot2)
```

## Question 1
### Problem
Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^2$(1), (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case, test H0 : $\mu$ = $\mu_0$ vs H0 : $\mu$ $\ne$ $\mu_0$ , where $\mu_0$ is the mean of $\chi^2$(1), Uniform(0,2), and Exponential(1), respectively.

### Solution
```{r Q1}
#Part a: looking at a type I error
#all follow a similar algorithm as shown on page 193 in SCRR
n <- 100 #number of replicates
a <- 0.05 #significance level alpha
muA <- mean(rchisq(n, df = 1)) #mu0 in part a
muB <- mean(runif(n, 0, 2)) #mu0 in part b
muC <- mean(rexp(n, rate = 1)) #mu0 in part c
#become alternatives in the estimate power of a test (b).

m <- 1000 #number of replicates
pA <- numeric(m)
pB <- numeric(m)
pC <- numeric(m)

for(i in 1:m){
  xA <- rchisq(n, df = 1) #sample dist part a
  xB <- runif(n, 0, 2) #sample dist part b
  xC <- rexp(n, rate = 1) #sample dist part c
  
  ttestA <- t.test(xA, alternative = "two.sided",mu = muA)
  ttestB <- t.test(xB, alternative = "two.sided",mu = muB)
  ttestC <- t.test(xC, alternative = "two.sided",mu = muC)
  
  pA[i] <- ttestA$p.value
  pB[i] <- ttestB$p.value
  pC[i] <- ttestC$p.value
}

pHatA <- mean(pA <= a)
pHatB <- mean(pB <= a)
pHatC <- mean(pC <= a)

seHatA <- sqrt(pHatA * (1- pHatA)/m)
seHatB <- sqrt(pHatB * (1- pHatB)/m)
seHatC <- sqrt(pHatC * (1- pHatC)/m)
#Means:
cat("The means (of the p-values) are: ", c(pHatA, pHatB, pHatC), "\n")
#SE:
cat("The standard errors (of the p-values) are: ", c(seHatA, seHatB, seHatC))

#Part b: Estimating power of a test and outputting empirical power curves of the t-test from the three previous situations
#initial data array
mu1 <- c(seq(0,2,1/20))
MC <- length(mu1)
powA <- numeric(MC)
powB <- numeric(MC)
powC <- numeric(MC)
powNorm <- numeric(MC)

#select theta_1 from the parameter subspace
#muA, muB, and muC values
#set for loop
for(j in 1:MC){
  #Chi Squared Distribution
  pvalA <- replicate(MC, expr = {
    xA <- rchisq(n, df = 1) #sample dist part a
    ttestA <- t.test(xA, alternative = "two.sided",mu = mu1[j])
    ttestA$p.value
  })
  powA[j] <- mean(pvalA <= a)
  #Uniform Distribution
  pvalB <- replicate(MC, expr = {
    xB <- runif(n, 0, 2) #sample dist part b
    ttestB <- t.test(xB, alternative = "two.sided",mu = mu1[j])
    ttestB$p.value
  })
  powB[j] <- mean(pvalB <= a)
  #Exponential Distribution
  pvalC <- replicate(MC, expr = {
    xC <- rexp(n, rate = 1) #sample dist part c
    ttestC <- t.test(xC, alternative = "two.sided",mu = mu1[j])
    ttestC$p.value
  })
  powC[j] <- mean(pvalC <= a)
  #Normal Distribution using power.t.test function
  powerTest <- power.t.test(n = MC, delta = mu1[j],sig.level = a,alternative = "two.sided")
  powNorm[j] <- powerTest$power
  
}

#making of the data frame
mean <- c(mu1,mu1,mu1,mu1)
powerR <- c(powNorm, powA, powB, powC)
distType <- c(replicate(MC, "Normal"),replicate(MC, "Chi Squared"),replicate(MC, "Uniform"),replicate(MC, "Exponential"))

data <- data.frame(mean, powerR,distType)

ggplot(data = data, aes(x = mean, y = powerR, color = distType)) +
  geom_point() +
  labs(x = 'Mean', y = 'Power', title = 'Empirical Power Test') +
  geom_errorbar(data = data, mapping = aes(x = mean, ymin = powerR - 2*(sqrt(powerR * (1- powerR)/MC)), ymax = powerR + 2*(sqrt(powerR * (1- powerR)/MC))))

```

\newpage
## Question 2

### Part a and b: MC and IS

For part (a) and (b), we are comparing Monte Carlo method and Importance Sampling (IS) to estimate $\alpha$ using the Z-test. In order to do Importance Sampling in part (b), we must first derive the weights: $f(X)/g(X)$. The problem statement tells us that the function $g(x) = Pois(1.5\lambda)$, which means the importance function, $f(x)$, is also based on the Poisson pdf. Therefore, 
$$f(x) = \frac{e^{\lambda}\lambda^x}{x!}$$
and
$$g(x) = \frac{e^{-1.5\lambda}1.5\lambda^x}{x!}$$

```{r 2a}
set.seed(475)
#part a estimate alpha using MCEM
n <- 100 # distribution size
m <- 100 #Monte Carlo sample size
l <- 2 #lambda is equal to 2
#z test with Monte Carlo
ztest <- replicate(m, expr = {
  x <- rpois(n, l) #samples
  ztest <- (mean(x)-2)/(sd(x)/sqrt(n))
})
#alpha estimate
aMC <- mean(ztest > 2.326)
cat("Alpha estimate using MC:", aMC, "\n")

#part b: estimate alpha using Importance Sampling
g <- function(x) (exp(-1.5*l)*((1.5*l)^x))/factorial(x)
f <- function(x) (exp(-l)*(l^x))/factorial(x)
phi <- function(x) as.numeric((mean(x)-2)/(sd(x)/sqrt(n))  > 2.326)
weight <- function(x) f(x) / g(x)
out <- numeric(m)

out <- replicate(m, expr = {
  x <- rpois(n, l) #samples
  out <- phi(x)*weight(x)
})

isOut <- mean(out)
cat("Alpha estimate using IS:", isOut,"\n")

```

### Part c: Which is better?
Both outputs of the function are off from the expected 0.05 estimate of $\alpha$. I would argue that the Monte Carlo is easier to implement because the weights do not need to be derived. However, IS estimates the $\alpha$ value closer to the expected value, compared to using Monte Carlo by itself. 

\newpage
## Question 3

```{r Boostrap with Residuals}
set.seed(475)
#get data
salmon <-read.table("salmon.dat", header = TRUE)
#initalize values
n <- length(salmon$recruits)
y <- 1/salmon$recruits
x <- 1/salmon$spawners
B <- 10000
beta.hat <- rep(0,B)
#get the linear model and coefficients and first set of residuals
linear.model <- lm(y~x)
#calculate coefficients (slope and intercept)
beta.true.r <- (1-linear.model$coef[2])/linear.model$coef[1]
beta <- (1-linear.model$coef[2])/linear.model$coef[1]
eps <- y - beta #residual error

#Bootstrapping the residuals
for(b in 1:B){
  eps.new <- eps[sample(1:n,n,replace = TRUE)]
  yB <- eps.new + beta
  fit <- lm(yB ~ x)
  beta.hat[b] <- (1-fit$coef[2])/fit$coef[1]
}
#output
hist(beta.hat,breaks = 25, main = "Beta from Boostrap with Residuals", xlab = "Beta Estimate")
print("95% CI Beta estimate from Boostrap with Residuals")
quantile(beta.hat,probs = c(0.025,0.975),na.rm=T)

#part b: Jackknife after bootstrap
se.beta <- rep(0,B)
for(b in 1:B){
  se.beta[b] <- sd(beta.hat[-b])
}
se.beta.bar <- mean(se.beta)
se.beta.jack <- (B-1)/B*sum((se.beta-se.beta.bar)^2)
cat("Bias Corrected Estimate:", (B-1)*(se.beta.bar-sd(beta.hat)),"\n")
cat("SE from Jackknife-after-bootstrap:", se.beta.jack)

```

```{r Bootstrap the cases}
#Bootstrapping the Cases
set.seed(475)
#get data
salmon <-read.table("salmon.dat", header = TRUE)
x <- 1/(salmon$spawners)
y <- 1/(salmon$recruits)
n <- length(x)
B <- 10000
beta.new <- rep(0,B)
model.new <- lm(y ~ x)
beta.true.c <- as.numeric((1-model.new$coef[2])/model.new$coef[1])
for(b in 1:B){
  j <- sample(1:n,n,replace = TRUE)
  x.new <- x[j]
  y.new <- y[j]
  model.new <- lm(y.new~x.new)
  beta.new[b] <- (1-model.new$coef[2])/model.new$coef[1]
}
hist(beta.new,breaks = 25, main = "Beta from Boostrap with Cases", xlab = "Beta Estimate")
ci.95.case <- quantile(beta.new,probs = c(0.025,0.975))
print("95% CI Beta estimate from Boostrap with Cases")
ci.95.case

#part b: Jackknife after bootstrap
se.beta <- rep(0,B)
m.beta <- rep(0,B)
for(b in 1:B){
  se.beta[b] <- sd(beta.new[-b])
}
se.beta.bar <- mean(se.beta)
se.beta.jack <- (B-1)/B*sum((se.beta-se.beta.bar)^2)
cat("Bias Corrected Estimate:", (B-1)*(se.beta.bar-sd(beta.new)),"\n")
cat("SE from Jackknife-after-bootstrap:", se.beta.jack)

```

\newpage

## Question 4
```{r Cancer}
set.seed(475)
#get data
cancer <- read.table("cancersurvival.dat", header = TRUE) #1 is stomach cancer, 2 is breast cancer
#initalize
n <- length(cancer[,1])
theta <- NULL
thetas <- rep(0,B)
theta.hat <- rep(0,n)
psi <- rep(0,n)
#Bootstrap!!
model <- lm(cancer[,2]~cancer[,1])
theta <- as.numeric(model$coefficients[2]/model$coefficients[1])
for(i in 1:B){
  cancer.new <- cancer[sample(1:n,n,replace = TRUE),]
  model.new <- lm(cancer.new[,2]~cancer.new[,1])
  thetas[i] <- as.numeric(model.new$coefficients[2]/model.new$coefficients[1])
}
ci.95 <- quantile(thetas,c(0.025,0.975),na.rm=T)
#BCA
for(j in 1:n){
  mod = lm(cancer[-j,2] ~ cancer[-j,1])
  theta.hat[j] <- as.numeric(mod$coefficients[2]/mod$coefficients[1])
}
for(k in 1:n){
  psi[k] <- mean(theta.hat[-k])-theta.hat[k]
}
a <- ((1/6)*sum(psi^3))/((sum(psi^2))^(3/2))
b <- qnorm(mean(thetas<theta),0,1)
beta1 <- pnorm(b + (b+qnorm(.025,0,1))/(1-a*(b+qnorm(.025,0,1))))
beta2 <- pnorm(b + (b+qnorm(.975,0,1))/(1-a*(b+qnorm(.975,0,1))))
ci.95.bc = quantile(thetas,c(beta1,beta2),na.rm=T)

#output
theta #observed estimate theta
ci.95 #95% ci from basic bootstrap
ci.95.bc #95% ci from BCA
hist(thetas,breaks=40)

#Bootstrap t



```

\newpage
## Question 5

