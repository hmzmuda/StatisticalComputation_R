---
title: "Math 475: Final Exam"
author: "Hannah Zmuda"
date: "01/08/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
### part a 
**Show the outputs of the EM algorithm are consistent with the given parameter equations**
To find the updated parameters (i.e. the maximized value) we fist need to find the Q-function (E step) then maximize the Q function by taking the derivative in regard to each of the parameters (M step).
**E step:**
Given the likelihood equation, we can work out the log likelihood to be:
$$log[L(\theta | n_{k,i})] = \Sigma^{16}_{i = 0} z_{0}log(\alpha 1_{i = 0}) + $$
$$(t_i)[log(\alpha 1_{i = 0}) + log(\beta \mu^i e^{-\mu}) + log((1-\alpha-\beta)\lambda^ie^{-\lambda})] + $$
$$(p_i)[log(\alpha 1_{i = 0}) + log(\beta \mu^i e^{-\mu}) + log((1-\alpha-\beta)\lambda^ie^{-\lambda})] - log(i!)$$
where $y$ is the complete data set and $z_0,t_i, p_i$ represent three different groups. These are further broken down into the zero, typical, and promiscuous groups.
To find your Q-function, take the expectation of the log likelihood function:

$$Q(\theta | \theta^{(t)}) = n_{z,0}^{(t)}\Sigma_{i = 0}^{16}log(\alpha 1_{i = 0}) + n_{t,i}^{(t)}\Sigma_{i = 0}^{16}log(\beta \mu^i exp(-\mu)) + n_{p,i}^{(t)}\Sigma_{i = 0}^{16}log((1-\alpha-\beta)\lambda^iexp(\lambda))$$
**M step:**
For the M step of the EM algorithm, we need to maximize the Q function in regard to each parameter then set it equal to zero.

When the derivative is set equal to zero, we find that the updated parameters equal to what we expected:
$$\alpha^{(t+1)} = \frac{n_0z_0{\theta^t}}{N}$$
$$\beta^{(t+1)} = \Sigma^{16}_{i=0} \frac{n_it_i(\theta^{(t)})}{N}$$
$$\mu^{(t+1)} = \frac{\Sigma^{16}_{i=0} i n_i t_i(\theta^{(t)})}{\Sigma^{16}_{i = 0}n_it_i(\theta^{(t)})}$$
$$\lambda^{(t+1)} = \frac{\Sigma^{16}_{i=0} i n_i p_i(\theta^{(t)})}{\Sigma^{16}_{i = 0}n_ip_i(\theta^{(t)})}$$
### part b and c
```{r Problem 1}
set.seed(475)
#initialize variables
data = data.frame(enc=0:16,
               freq=c(379,299,222,145,109,95,73,59,45,30,24,12,4,2,0,1,1))
N = sum(data$freq)
y = rep(data$enc,data$freq)
alpha = 0.5
beta = 0.8
mu = 2
lambda = 15
param = c(alpha,beta,mu,lambda)
param.guess = c(0.1,0.2,3,4)
tol = 1e-10
tol.cur = 100
time = 0
i = 0:16
#functions
log.likelihood <- function(alpha,beta,mu,lambda,x){
  l = 0
  for(i in 1:length(x$enc))
  {
    e = x$enc[i]
    n = x$freq[i]
    pi = (beta*exp(-mu)*mu^i) + ((1-alpha-beta)*exp(-lambda)*lambda^i)
    pi[1] = pi[1] + alpha
    z.stat = alpha/(pi[1])
    t.stat = (beta*(mu^i)*exp(-mu))/pi
    p.stat = ((1-alpha-beta)*exp(-lambda)*lambda^i)/pi
    
    if(e==0)
      l = l + n*z.stat*log(alpha) + n*t.stat*log(beta*exp(-mu)) + n*p.stat*log((1-alpha-beta)*exp(-lambda))
    else
      l = l + n*t.stat*log(beta*exp(-mu)*mu^e) + n*p.stat*log((1-alpha-beta)*exp(-lambda)*lambda^e)-log(gamma(e+1))
  }
  return(l)
}

#EM Algorithm
while(tol.cur > tol){
  pi = (beta*exp(-mu)*mu^i) + ((1-alpha-beta)*exp(-lambda)*lambda^i)
  pi[1] = pi[1] + alpha
  z.stat = alpha/(pi[1])
  t.stat = (beta*(mu^i)*exp(-mu))/pi
  p.stat = ((1-alpha-beta)*exp(-lambda)*lambda^i)/pi
  alpha = (data$freq[1]*z.stat)/N
  beta = sum(data$freq*t.stat)/N
  mu = sum(i*data$freq*t.stat)/sum(data$freq*t.stat)
  lambda = sum(i*data$freq*p.stat)/sum(data$freq*p.stat)
  new.param = c(alpha,beta,mu,lambda)
  tol.cur = sum(abs(new.param-param))
  param = new.param
  time = time + 1
}
param

hist(rep(data$enc,data$freq),breaks=-0.5+c(0:17),freq=F,main = "Histogram of Risky Sexual Encounters",xlab = "Encounters")
#hist(y,freq=F)
z = 0:16
prob = (beta*exp(-mu)*mu^z + (1-alpha-beta)*exp(-lambda)*lambda^z)/(factorial(z))
prob[1] = prob[1]+alpha
for(i in 1:length(z)){
  lines(c(z[i]-0.1,z[i]-0.1),c(0,prob[i]),lwd=5,col=1)
}

pois.hat = mean(y)

for(i in 1:length(z)){
  lines(c(z[i]+0.1,z[i]+0.1),c(0,dpois(z[i],pois.hat)),lwd=5,col=2)
}

legend("topright",c("Poisson mixtures","Poisson"),lty=1,col=1:2)

```

```{r Problem 1c}
#standard error
#Use log likelihood at theta.hat values (i.e. new parameter values)
set.seed(1234)
data = data.frame(enc=0:16,
               freq=c(379,299,222,145,109,95,73,59,45,30,24,12,4,2,0,1,1))
tol = 1e-10
B = 1000
result.boot <- NULL
alpha <- 1
beta <- 2
mu <- 4
lambda <- 10
param <- c(alpha,beta,mu,lambda)

for(j in 1:B){
  #randomize samples
  data.boot <- rmultinom(1,sum(data$freq),prob = data$freq/n)
  #set initial values
  tol.cur <- 100
  N <- sum(data.boot)
  i = c(0:16)
  #loop
  while(tol.cur > tol){
    pi = (beta*exp(-mu)*mu^i) + ((1-alpha-beta)*exp(-lambda)*lambda^i)
    pi[1] = pi[1] + alpha
    z.stat = alpha/(pi[1])
    t.stat = (beta*(mu^i)*exp(-mu))/pi
    p.stat = ((1-alpha-beta)*exp(-lambda)*(lambda^i))/pi
    
    alpha = (data.boot[1]*z.stat)/N
    beta = sum(data.boot*t.stat)/N
    mu = sum(i*data.boot*t.stat)/sum(data.boot*t.stat)
    lambda = sum(i*data.boot*p.stat)/sum(data.boot*p.stat)
    
    new.param = c(alpha,beta,mu,lambda)
    tol.cur = sum(abs(new.param-param))
    param = new.param
  }
  result.boot <- rbind(result.boot,param)
}
result.boot[B,]
cov(result.boot) #covariance matrix to show standard error
#pairwise correlation
cor(result.boot)
```

\newpage

# Problem 2
## part a: Metropolis and M-H Algorithm
We are seeking to estimate $\alpha, \beta, \mu, \lambda$ using the Metropolis Algorithm.

```{r Problem 2a}
#Metropolis Samples (Symmetric Proposal Distribution)
set.seed(575)
#initialize variables and constants
data = data.frame(enc=0:16,freq=c(379,299,222,145,109,95,73,59,45,30,24,12,4,2,0,1,1))
N = sum(data$freq)
y = rep(data$enc,data$freq)
reject <- 0
alpha = rep(0,N)
beta = rep(0,N)
mu = rep(0,N)
lambda = rep(0,N)
minn = -2
maxx = 2

#functions
target <- function(alpha,beta,mu,lambda,x){
  l = 0
  for(j in 1:length(x$enc))
  {
    e = x$enc[j]
    n = x$freq[j]
    pi = (beta*exp(-mu)*mu^e) + ((1-alpha-beta)*exp(-lambda)*lambda^e)
    pi[1] = pi[1] + alpha
    z.stat = alpha/pi[1]
    t.stat = (beta*(mu^e)*exp(-mu))/pi
    p.stat = ((1-alpha-beta)*exp(-lambda)*lambda^e)/pi

    if(e==0){
      l = l * ((z.stat)^(n) + (t.stat)^(n) + (p.stat)^(n))
    }

    else{
      l = l * ((t.stat)^(n) + (p.stat^(n)))/(factorial(e))
    }
  }
  return(l) 
}

#initialize samples
alpha[1] = 0.5
beta[1] = 2
mu[1] = 3
lambda[1] = 5
i = 1
for(i in 2:N){

  #sample from proposal distribution (symmetrical)
  alpha.star <- alpha[i-1] + runif(1,minn,maxx)
  beta.star <- beta[i-1] + runif(1,minn,maxx)
  mu.star <- mu[i-1] + runif(1,minn,maxx)
  lambda.star <- lambda[i-1] + runif(1,minn,maxx)
  
  ratio <- target(alpha.star,beta.star,mu.star,lambda.star,data)/target(alpha[i-1],beta[i-1],mu[i-1],lambda[i-1],data)
  accept.prob <- min(1,ratio)
  
  U = runif(1)
  
  if(U < accept.prob){
    alpha[i] <- alpha.star
    beta[i] <- beta.star
    mu[i] <- mu.star
    lambda[i] <- lambda.star
  }
  else{
    alpha[i] <- alpha[i-1]
    beta[i] <- beta[i-1]
    mu[i] <- mu[i-1]
    lambda[i] <- lambda[i-1]
    reject = reject + 1
  }
  c(alpha[i],beta[i],mu[i],lambda[i])
}
c(mean(alpha),mean(beta),mean(mu),mean(lambda))
print("Rejection Rate:")
100*(reject/N)

```

## part b: MCMH
```{r Problem 2b}
#Metropolis-Hastings Algorithm
set.seed(575)
#initialize variables and constants
data = data.frame(enc=0:16,freq=c(379,299,222,145,109,95,73,59,45,30,24,12,4,2,0,1,1))
N = sum(data$freq)
y = rep(data$enc,data$freq)
reject <- 0
alpha = rep(0,N)
beta = rep(0,N)
mu = rep(0,N)
lambda = rep(0,N)
mean = 0
sigma = 0.5
minn = -5
maxx = 5

#functions
like <- function(alpha,beta,mu,lambda,x){
  l = 0
  for(i in 1:length(x$enc))
  {
    e = x$enc[i]
    n = x$freq[i]
    
    if(e==0){
      l = l * ((alpha)^(n) + (beta*exp(-mu))^(n) + ((1-alpha-beta)*exp(-lambda))^(n))
    }

    else{
      l = l * ((beta*exp(-mu)*mu^e)^(n) + ((1-alpha-beta)*exp(-lambda)*lambda^e)^(n))/(factorial(e))
    }
  }
  return(l) 
}

#initialize samples
alpha[1] = runif(1,minn,maxx)
beta[1] = runif(1,minn,maxx)
mu[1] = runif(1,minn,maxx)
lambda[1] = runif(1,minn,maxx)
i = 1

for(i in 2:N){

  #sample from proposal distribution (non-symmetrical)
  alpha.star <- rnorm(1,alpha[i-1],sigma)
  beta.star <- rnorm(1,beta[i-1],sigma)
  mu.star <- rnorm(1,mu[i-1],sigma)
  lambda.star <- rnorm(1,lambda[i-1],sigma)
  ratio <- like(alpha.star,beta.star,mu.star,lambda.star,data)/like(alpha[i-1],beta[i-1],mu[i-1],lambda[i-1],data)
  accept.prob <- min(1,ratio)
  
  U <- runif(1) #uniform distribution
  
  if(U < accept.prob){
    alpha[i] <- alpha.star
    beta[i] <- beta.star
    mu[i] <- mu.star
    lambda[i] <- lambda.star
  }
  else{
    alpha[i] <- alpha[i-1]
    beta[i] <- beta[i-1]
    mu[i] <- mu[i-1]
    lambda[i] <- lambda[i-1]
    reject = reject + 1
  }
}
c(mean(alpha),mean(beta),mean(mu),mean(lambda))
print("Rejection Rate:")
100*(reject/N)

```

\newpage
# Problem 3

\newpage 
# Problem 4

\newpage 
# Problem 5
```{r Problem 5}
set.seed(775)
n <- 1000 #sample size
comp <- sample(1:2, prob = c(0.8,0.2), size = n, replace = TRUE)
mix <- 0.8*rnorm(n,0,1) + 0.2*rnorm(n,3,1)
mu = c(0,3)
stan.dev = c(1,1)
samp <- rnorm(n, mean = mu[comp], sd = stan.dev[comp])

h1 <- 1.06*(n^(-1/5))*sd(samp)
h2 <- 0.9*(n^(-1/5))*min(sd(samp),(IQR(samp))/1.34)
k.gauss <- function(t){return((1/sqrt(2*pi))*-0.5*t^2)}

plot(density(samp),col = 1,main = "Gaussian Kernel Density", lwd = 2) #true density estimate
lines(density(samp, bw = h1, kernel = "gaussian"),col = 2)
lines(density(samp, bw =h2, kernel = "gaussian"),col = 3)
legend("topright",c("True Density Mixture","h1 Bandwidth","h2 Bandwidth"),lty=1,col=1:3)

#part b
plot(density(samp, kernel = "epanechnikov"),col = 1,main = "Epanechnikov Kernel Density", lwd = 2) #true density estimate
lines(density(samp, bw = h1, kernel = "epanechnikov"),col = 2)
lines(density(samp, bw =h2, kernel = "epanechnikov"),col = 3)
legend("topright",c("True Density Mixture","h1 Bandwidth","h2 Bandwidth"),lty=1,col=1:3)

plot(density(samp, kernel = "rectangular"),col = 1,main = "Rectangular Kernel Density",lwd = 2) #true density estimate
lines(density(samp, bw = h1, kernel = "rectangular"),col = 2)
lines(density(samp, bw =h2, kernel = "rectangular"),col = 3)
legend("topright",c("True Density Mixture","h1 Bandwidth","h2 Bandwidth"),lty=1,col=1:3)

plot(density(samp, kernel = "triangular"),col = 1,main = "Triangular Kernel Density", lwd = 2) #true density estimate
lines(density(samp, bw = h1, kernel = "triangular"),col = 2)
lines(density(samp, bw =h2, kernel = "triangular"),col = 3)
legend("topright",c("True Density Mixture","h1 Bandwidth","h2 Bandwidth"),lty=1,col=1:3)

```
Based on the plots from above. The h2 bandwidth is a better smoothing parameter compared to using the h1 bandwidth. 
