---
title: "Math 475: Final Exam"
author: "Hannah Zmuda"
date: "01/08/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

# Problem 1
## part a 
**Show the outputs of the EM algorithm are consistent with the given parameter equations**
To find the updated parameters (i.e. the maximized value) we fist need to find the Q-function (E step) then maximize the Q function by taking the derivative in regard to each of the parameters (M step).
**E step:**
Given the observed likelihood, we can compute the complete likelihood to be:
$$L(\theta | n_{k,i}) = \Pi^{16}_{i = 0}\frac{[z(\theta)^{(n_0)} t(\theta)^{n_i}p(\theta)^{n_i}]}{i!}$$
Given the likelihood equation, we can work out the log likelihood to be:
$$log[L(\theta | n_{k,i})] = \Sigma^{16}_{i = 0} z_{0}log(\alpha 1_{i = 0}) + $$
$$(t_i)[log(\alpha 1_{i = 0}) + log(\beta \mu^i e^{-\mu}) + log((1-\alpha-\beta)\lambda^ie^{-\lambda})] + $$
$$(p_i)[log(\alpha 1_{i = 0}) + log(\beta \mu^i e^{-\mu}) + log((1-\alpha-\beta)\lambda^ie^{-\lambda})] - log(i!)$$
where $y$ is the complete data set and $z_0,t_i, p_i$ represent three different groups. These are further broken down into the zero, typical, and promiscuous groups.
To find your Q-function, take the expectation of the log likelihood function:

$$Q(\theta | \theta^{(t)}) = n_{z,0}^{(t)}\Sigma_{i = 0}^{16}log(\alpha 1_{i = 0}) + n_{t,i}^{(t)}\Sigma_{i = 0}^{16}log(\beta \mu^i exp(-\mu)) + n_{p,i}^{(t)}\Sigma_{i = 0}^{16}log((1-\alpha-\beta)\lambda^iexp(\lambda))$$
**M step:**
For the M step of the EM algorithm, we need to maximize the Q function in regard to each parameter then set it equal to zero.

When the derivative is set equal to zero, we find that the updated parameters equal to what we expected:
$$\alpha^{(t+1)} = \frac{n_0z_0({\theta^{(t)}t})}{N}$$
$$\beta^{(t+1)} = \Sigma^{16}_{i=0} \frac{n_it_i(\theta^{(t)})}{N}$$
$$\mu^{(t+1)} = \frac{\Sigma^{16}_{i=0} i n_i t_i(\theta^{(t)})}{\Sigma^{16}_{i = 0}n_it_i(\theta^{(t)})}$$
$$\lambda^{(t+1)} = \frac{\Sigma^{16}_{i=0} i n_i p_i(\theta^{(t)})}{\Sigma^{16}_{i = 0}n_ip_i(\theta^{(t)})}$$

## part b and c

```{r Problem 1}
set.seed(475)
#initialize variables
data = data.frame(enc=0:16,
               freq=c(379,299,222,145,109,95,73,59,45,30,24,12,4,2,0,1,1))
N = sum(data$freq)
y = rep(data$enc,data$freq)
alpha = 0.5
beta = 0.8
mu = 2
lambda = 15
param = c(alpha,beta,mu,lambda)
param.guess = c(0.1,0.2,3,4)
tol = 1e-10
tol.cur = 100
time = 0
i = 0:16
#functions
log.likelihood <- function(alpha,beta,mu,lambda,x){
  l = 0
    alpha = exp(alpha)/(1+exp(alpha))
  beta = exp(beta)/(1+exp(beta))
  mu = exp(mu)/(1+exp(mu))
  lambda = exp(lambda)/(1+exp(lambda))
  for(i in 1:length(x$enc))
  {
    e = x$enc[i]
    n = x$freq[i]
    if(e==0){
      l = l + n*log   (alpha + beta*exp(-mu) + (1-alpha-beta)*exp(-lambda))
      print(l)
    }
    else{
      l = l + n*log(beta*(mu^e)*exp(-mu) + (1-alpha-beta)*exp(-lambda)*lambda^e)-log(factorial(e))
      print(l)
    }
  }
  return(l)
}

#EM Algorithm
while(tol.cur > tol){
  pi = (beta*exp(-mu)*mu^i) + ((1-alpha-beta)*exp(-lambda)*lambda^i)
  pi[1] = pi[1] + alpha
  z.stat = alpha/(pi[1])
  t.stat = (beta*(mu^i)*exp(-mu))/pi
  p.stat = ((1-alpha-beta)*exp(-lambda)*lambda^i)/pi
  alpha = (data$freq[1]*z.stat)/N
  beta = sum(data$freq*t.stat)/N
  mu = sum(i*data$freq*t.stat)/sum(data$freq*t.stat)
  lambda = sum(i*data$freq*p.stat)/sum(data$freq*p.stat)
  new.param = c(alpha,beta,mu,lambda)
  tol.cur = sum(abs(new.param-param))
  param = new.param
  time = time + 1
}
param

hist(rep(data$enc,data$freq),breaks=-0.5+c(0:17),freq=F,main = "Histogram of Risky Sexual Encounters",xlab = "Encounters")
#hist(y,freq=F)
z = 0:16
prob = (beta*exp(-mu)*mu^z + (1-alpha-beta)*exp(-lambda)*lambda^z)/(factorial(z))
prob[1] = prob[1]+alpha
for(i in 1:length(z)){
  lines(c(z[i]-0.1,z[i]-0.1),c(0,prob[i]),lwd=5,col=1)
}

pois.hat = mean(y)

for(i in 1:length(z)){
  lines(c(z[i]+0.1,z[i]+0.1),c(0,dpois(z[i],pois.hat)),lwd=5,col=2)
}

legend("topright",c("Poisson mixtures","Poisson"),lty=1,col=1:2)

```

```{r Problem 1c}
#standard error
#Use log likelihood at theta.hat values (i.e. new parameter values)
set.seed(1234)
data = data.frame(enc=0:16,
               freq=c(379,299,222,145,109,95,73,59,45,30,24,12,4,2,0,1,1))
tol = 1e-10
B = 1000
result.boot <- NULL
alpha <- 1
beta <- 2
mu <- 4
lambda <- 10
param <- c(alpha,beta,mu,lambda)

for(j in 1:B){
  #randomize samples
  data.boot <- rmultinom(1,sum(data$freq),prob = data$freq/length(data$freq))
  #set initial values
  tol.cur <- 100
  N <- sum(data.boot)
  i = c(0:16)
  #loop
  while(tol.cur > tol){
    pi = (beta*exp(-mu)*mu^i) + ((1-alpha-beta)*exp(-lambda)*lambda^i)
    pi[1] = pi[1] + alpha
    z.stat = alpha/(pi[1])
    t.stat = (beta*(mu^i)*exp(-mu))/pi
    p.stat = ((1-alpha-beta)*exp(-lambda)*(lambda^i))/pi
    
    alpha = (data.boot[1]*z.stat)/N
    beta = sum(data.boot*t.stat)/N
    mu = sum(i*data.boot*t.stat)/sum(data.boot*t.stat)
    lambda = sum(i*data.boot*p.stat)/sum(data.boot*p.stat)
    
    new.param = c(alpha,beta,mu,lambda)
    tol.cur = sum(abs(new.param-param))
    param = new.param
  }
  result.boot <- rbind(result.boot,param)
}
result.boot[B,]
cov(result.boot) #covariance matrix to show standard error
#pairwise correlation
cor(result.boot)
```

\newpage

# Problem 2
## part a: Metropolis and M-H Algorithm
We are seeking to estimate $\alpha, \beta, \mu, \lambda$ using the Metropolis Algorithm.

Because the proposal distribution is symmetric, it can be canceled out in the ratio calculation, and we can then focus on the ratio of the target distribution with theta star and the previous theta value.

## part 2: MCMH

```{r Problem 2a}
set.seed(5)
df <- c(379,299,222,145,109,95,73,59,45,30,24,12,4,2,0,1,1)
alpha <- 0.23
beta <- 0.25
mu <- 3.5
lambda <- 3.5
N <- 1000
chain <- matrix(nrow = N, ncol = 4)
chain[1,] <- c(alpha,beta,mu,lambda)
minn = -0.5
maxx = 0.5
reject = 0
count = 0
pi_i <- function(alpha,beta,mu,lambda,i){
  if(i==0) return(alpha + beta*exp(-mu) + (1-alpha-beta)*exp(-lambda))
  else return(beta*(mu^i)*exp(-mu) + (1-alpha-beta)*(lambda^i)*exp(-lambda))
}
loglike <- function(alpha,beta,mu,lambda){
  sum.out <- 0
  alpha = exp(alpha)/(1+exp(alpha))
  beta = exp(beta)/(1+exp(beta))
  mu = exp(mu)
  lambda = exp(lambda)
  for(i in 1:length(df)){
    sum.out <- sum.out + df[i]*(log(pi_i(alpha,beta,mu,lambda,i-1)) - log(factorial(i-1)))
  }
  return(sum.out)
}
options(warn = -1)
for(i in 2:N){
  alpha.star <- chain[i-1,1] + runif(1,minn,maxx)
  while(alpha.star < 0 || alpha.star > 1){
    alpha.star <- chain[i-1,1] + runif(1,minn,maxx)
  }
  beta.star <- chain[i-1,2] + runif(1,minn,maxx)
  while(beta.star < 0 || beta.star > 1){
    beta.star <- chain[i-1,2] + runif(1,minn,maxx)
  }
  mu.star <- chain[i-1,3] + runif(1,-0.1,0.1)
  lambda.star <- chain[i-1,4] + runif(1,-0.1,0.1)
  ratio <- exp(loglike(alpha.star,beta.star,mu.star,lambda.star) - loglike(chain[i-1,1],chain[i-1,2],chain[i-1,3],chain[i-1,4]))
  options(warn = -1)
  if(is.nan(ratio)){ratio = 0}
  if(runif(1) < ratio){
    chain[i,] <- c(alpha.star,beta.star,mu.star,lambda.star)
  }
  else{
    chain[i,] <- chain[i-1,]
    reject <- reject + 1
  }
  count = count +1
}
options(warn = -1)
c(mean(chain[,1]),mean(chain[,2]),mean(chain[,3]),mean(chain[,4]))
print("Rejection Rate:")
100*(reject/count)

#Bootstrap the variances


```

```{r Problem 2b}
#Metropolis Hastings
set.seed(5)
df <- c(379,299,222,145,109,95,73,59,45,30,24,12,4,2,0,1,1)
alpha <- 0.23
beta <- 0.25
mu <- 3.5
lambda <- 3.5
N <- 1000
chain <- matrix(nrow = N, ncol = 4)
chain[1,] <- c(alpha,beta,mu,lambda)
minn = -0.5
maxx = 0.5
reject = 0
pi_i <- function(alpha,beta,mu,lambda,i){
  if(i==0) return(alpha + beta*exp(-mu) + (1-alpha-beta)*exp(-lambda))
  else return(beta*(mu^i)*exp(-mu) + (1-alpha-beta)*(lambda^i)*exp(-lambda))
}
loglike <- function(alpha,beta,mu,lambda){
  sum.out <- 0
  alpha = exp(alpha)/(1+exp(alpha))
  beta = exp(beta)/(1+exp(beta))
  mu = exp(mu)
  lambda = exp(lambda)
  for(i in 1:length(df)){
    sum.out <- sum.out + df[i]*(log(pi_i(alpha,beta,mu,lambda,i-1)) - log(factorial(i-1)))
  }
  return(sum.out)
}
options(warn = -1)
for(i in 2:N){
  alpha.star <- chain[i-1,1] + rexp(1,1)
  while(alpha.star < 0 || alpha.star > 1){
    alpha.star <- chain[i-1,1] + rexp(1,1)
  }
  beta.star <- chain[i-1,2] + rexp(1,1)
  while(beta.star < 0 || beta.star > 1){
    beta.star <- chain[i-1,2] + rexp(1,1)
  }
  mu.star <- chain[i-1,3] + rexp(1,1)
  lambda.star <- chain[i-1,4] + rexp(1,1)
  ratio <- exp(loglike(alpha.star,beta.star,mu.star,lambda.star) - loglike(chain[i-1,1],chain[i-1,2],chain[i-1,3],chain[i-1,4]))
  options(warn = -1)
  if(is.nan(ratio)){ratio = 0}
  if(runif(1) < ratio){
    chain[i,] <- c(alpha.star,beta.star,mu.star,lambda.star)
  }
  else{
    chain[i,] <- chain[i-1,]
    reject <- reject + 1
  }
}
options(warn = -1)
c(mean(chain[,1]),mean(chain[,2]),mean(chain[,3]),mean(chain[,4]))
print("Rejection Rate:")
100*(reject/N)

#Bootstrap the variances


```



\newpage
# Problem 3
## part a:
From the given data for the clinical trial, we can see from the box-and-whisker plot that the Hormone group with the censored time has the most patients out of the four groups. Not only that, but the means are more spread apart within the hormone group (difference of 13.68) compared to the control group (difference of 7.2). Based on the combined Normal QQ plot, the Control Group with Censored time most closely follows a normal distribution ("3" from list.id legend). The two from the hormone group of Recurrence and Censor times (0 and 1, respectively), do not closely align with a normal distribution.
## part b: 
Given the likelihood and prior, we can use those to calculate the conditional distributions of $\theta$ and $\tau$. To do this we will first need to calculate the joint probability density function:
$$P(y|\theta,\tau) \propto L(\theta,\tau|y)f(\theta,\tau)$$
$$\pi(\theta,\tau|y) \propto \pi(\theta,\tau,y) = L(\theta,\tau|y)f(\theta,\tau)$$
$$P(\theta,\tau|y) = \theta^{\Sigma \delta_i^c + \Sigma\delta_i^H +2a}\tau^{\Sigma\delta_i^H + b}exp(-\theta(\Sigma x_i^C + c) - \tau\theta(\Sigma x_i^H + d))$$
With this we can calculate the conditional distributions for the parameters:
$$P(\tau | \theta,y) = \tau^{\Sigma\delta_i^H + b}exp(- \tau\theta(\Sigma x_i^H + d))$$
$$ \propto Gamma(\tau | \Sigma\delta_i^H + b + 1,\Sigma x_i^C \Sigma x_i^H + c + d$$
$$P(\theta|\tau,y) = \theta^{\Sigma \delta_i^c + \Sigma\delta_i^H + a}exp(-\theta(\Sigma x_i^C + c) - \tau\theta(\Sigma x_i^H + d))$$
$$ \propto Gamma(\theta | \Sigma \delta_i^c + \Sigma\delta_i^H + a + 1,\Sigma x_i^C + \tau \Sigma x_i^H + c + d\tau$$


```{r Problem 3}
set.seed(12345)
library(ggplot2)
library(Rmisc)
#initialize data sets
hormone.rec <- c(2,4,6,9,9,9,13,14,18,23,31,32,33,34,43)
lab.h.r <- rep(0,length(hormone.rec))
hormone.cens <- c(10,14,14,16,17,18,18,19,20,20,21,21,23,24,29,29,30,30,31,31,31,33,35,37,40,41,42,42,44,46,48,51,53,54,54,55,56)
lab.h.c <- rep(1,length(hormone.cens))
control.rec = c(1,4,6,7,13,24,25,35,35,39)
lab.c.r <- rep(2,length(control.rec))
control.cens = c(1,1,3,4,5,8,10,11,13,14,14,15,17,19,20,22,24,24,24,25,26,26,26,28,29,29,32,35,38,39,40,41,44,45,47,47,47,50,50,51)
lab.c.c <- rep(3,length(control.cens))
list.id <- c(lab.h.r,lab.h.c,lab.c.r,lab.c.c)
dat.df <- c(hormone.rec,hormone.cens,control.rec,control.cens)

cancer.dat <- data.frame(dat.df,list.id)
cancer.dat$list.id <- as.factor(cancer.dat$list.id)

#part a: Plots and quantiles
dat.list <- list(hormone.rec,hormone.cens,control.rec,control.cens)
boxplot(dat.list, range = 0.0, names = c("HG: Rec","HG: Cens","CG: Rec","CG: Cens"),xlab = "Patient Group",ylab = "Observed Months")

p <- qplot(sample = dat.df,data = cancer.dat, color = list.id, shape = list.id)
p + labs(title = "Months in Trial \n Based on Treatment Group", y = "Months in Trial")

ggplot(cancer.dat,aes(x = dat.df, fill = list.id, color = list.id))+ geom_histogram(position = "identity", fill = "transparent") + labs(title = "Histogram of Censored vs. Recurrence time in Test groups", x = "Time in Months to Second Recurrence")

#part c: Gibbs Sampler
#Initialize and prepare recurrence and censored data based on patient group
delta.h <- c(rep(1,length(hormone.rec)),rep(0,length(hormone.cens)))
delta.c <- c(rep(1,length(control.rec)),rep(0,length(control.cens)))
dat.hormone <- data.frame(x = c(hormone.rec,hormone.cens),delta.h)
dat.control <- data.frame(x = c(control.rec,control.cens),delta.c)
a = 3
b = 1
c = 60
d = 120

#FUNCTIONS
gibbs <- function(dat.control,dat.hormone,n,burn,hyperparameter){
  mat <- matrix(ncol = 2, nrow = n)
  tau <- 0.1
  theta <- 0.1
  mat[1,] <- c(theta,tau)
  a <- hyperparameter[1]
  b <- hyperparameter[2]
  c <- hyperparameter[3]
  d <- hyperparameter[4]
  for(i in 2:n){
    tau <- mat[i-1,2]
    one.tau <- a + 1 + sum(dat.control$delta.c) + sum(dat.hormone$delta.h)
    two.tau <-  sum(dat.control$x) + (tau*sum(dat.hormone$x)) + c + (d*tau)
    mat[i,1] <- rgamma(1, one.tau, two.tau)
    theta <- mat[i,1]
    one.theta <- b + 1 + sum(dat.hormone$delta.h)
    two.theta <- theta*sum(dat.hormone$x) + (theta*d)
    mat[i,2] <- rgamma(1, one.theta, two.theta)
  }
  burn <- burn + 1
  return(mat[burn:n,])
}

gibby <- gibbs(dat.control,dat.hormone,1000,100,c(3,1,60,120))
colMeans(gibby)
gibbies <- gibbs(dat.control,dat.hormone,5000,0,c(3,1,60,120))
colMeans(gibbies)
#Convergence diagnostics
par(mfrow=c(2,2))
plot(ts(gibbies[,1]),main = "Theta from 10000 Iterations",ylab = "Tau")
abline(h = mean(gibbies[,1]),col = "red")
plot(ts(gibbies[,2]),main = "Tau from 10000 Iterations",ylab = "Tau")
abline(h = mean(gibbies[,2]),col = "blue")
hist(gibbies[,1],40,main = "Theta from 10000 Iterations",xlab = "Theta Estimates")
hist(gibbies[,2],40,main = "Tau from 10000 Iterations",xlab = "Tau Estimates")

#part d: summary statistics
#marginal mean
marg.means <- colMeans(gibbies)
#sd
marg.sd <- c(sd(gibbies[,1]),sd(gibbies[,2]))
#95% Confidence interval
marg.ci.theta <- CI(gibbies[,1], ci = 0.95)
marg.ci.tau <- CI(gibbies[,2], ci = 0.95)

marg.stats <- matrix(c(marg.means[1],marg.means[2],marg.sd[1],marg.sd[2],marg.ci.theta[3],marg.ci.tau[3],marg.ci.theta[1],marg.ci.tau[1]),ncol = 2,byrow = TRUE)
colnames(marg.stats) <- c("Theta","Tau")
rownames(marg.stats) <- c("Marginal Mean", "Standard Deviation", "95% CI upper","95% CI lower")
as.table(marg.stats)

#part e: graphing the prior and posterior distributions
#Posterior is the theta and tau estimates from above with the prior being a gamma distribution
par(mfrow=c(1,1))
c1 <- rgb(173,216,230,max = 255, alpha = 80, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink")
theta.pos <- hist(gibbies[,1],breaks = 40,plot = FALSE)
tau.pos <- hist(gibbies[,2],breaks = 40,plot = FALSE)
theta.prior <- hist(dexp(quantile(gibbies[,1])),breaks = 40,plot = FALSE)
tau.prior <- hist(dgamma(quantile(gibbies[,2]),shape = 1 + 1 + sum(dat.hormone$delta.h),rate = gibbies[,1]*sum(dat.hormone$x) + (gibbies[,1]*120)),breaks = 120,plot = FALSE)
                  
plot(tau.pos, col = c1, main = "Histogram of Tau Posterior and Prior", ylim = range(1:750))
plot(tau.prior, col = c2, add = TRUE)

#part f: Interpreting result for the drug company
# Linear regression model to compare the theta and tau
scatter.smooth(gibbies[,2],gibbies[,1], xlab = "Theta",ylab = "Tau",main = "Theta ~ Tau")
cor(gibbies[,2],gibbies[,1])
#inverse correlation, indicating that there is not 

#part g: sensitivity analysis
hyp.half <- gibbs(dat.control,dat.hormone,10000,2500,c(1.5,2,30,60))
half.mean <- colMeans(hyp.half)
half.sd <- c(sd(hyp.half[,1]),sd(hyp.half[,2]))
half.ci.theta <- CI(hyp.half[,1], ci = 0.95)
half.ci.tau <- CI(hyp.half[,2], ci = 0.95)

hyp.dou <- gibbs(dat.control,dat.hormone,10000,2500,c(6,0.5,120,240))
dou.mean <- colMeans(hyp.dou)
dou.sd <- c(sd(hyp.dou[,1]),sd(hyp.dou[,2]))
dou.ci.theta <- CI(hyp.dou[,1], ci = 0.95)
dou.ci.tau <- CI(hyp.dou[,2], ci = 0.95)

#Summary statistics based on hyperparameters
hyp.table <- matrix(c(half.mean[1],half.mean[2],dou.mean[1],dou.mean[2],half.sd[1],half.sd[2],dou.sd[1],dou.sd[2],half.ci.theta[3],half.ci.tau[3],dou.ci.theta[3],dou.ci.tau[3],half.ci.theta[1],half.ci.tau[1],dou.ci.theta[1],dou.ci.tau[1]),ncol = 4,byrow = TRUE)
colnames(hyp.table) <- c("Theta, Halved", "Tau, Halved","Theta, Doubled", "Tau Doubled")
rownames(hyp.table) <- c("Marginal Mean", "Standard Deviation","CI 95% Lower","CI 95% Upper")
as.table(hyp.table)


```
## part f and g:

Based on the summary statistics produced in part d, I would say that the recurrence times are not statistically significant from the control group. This would mean that the clinical trial was not a huge success because the drug has very little affect on combating recurring episodes of breast cancer. We can see from the linear regression graph and the correlation output (-0.6475986) that the two parameters are inversely related.

For the hyperparameters, we can see from the table of summary statistics, that changing the hyperparameters affects our tau term more than the theta term. This can be seen in the 95% CI. This would be important to understand where these numbers come from and how to better estimate them before completing another clinical trial, because the hyperparameters could affect whether the drug is (statistically) effective.

\newpage 
# Problem 4
```{r Problem 4}
set.seed(475)

#part b: mean recurrence time
m.r.h <- mean(hormone.rec)
m.r.c <- mean(control.rec)
cat("Mean Recurrence Time for Hormone Group", m.r.h,"\n")
cat("Mean Recurrence Time for Control Group", m.r.c, "\n")

tau.est <- gibbies[,2]
theta.est <- gibbies[,1]
better <- c(0.032429,0.76652)
hormone.estimate <- rexp(length(theta.est),rate = mean(tau.est)*mean(theta.est))
control.estimate <- rexp(length(theta.est),rate = mean(theta.est))
cat("Mean Recurrence Time for Hormone Group", mean(hormone.estimate),"\n")
cat("Mean Recurrence Time for Control Group", mean(control.estimate), "\n")

#part c:
B = 1000 #Bootstrap Iteration
alpha <- 0.05
means.hormone <- rep(0,B)
means.control <- rep(0,B)
for(i in 1:B){
  means.hormone[i] <- mean(sample(hormone.estimate, size = 1:length(hormone.estimate), replace = TRUE))
  means.control[i] <- mean(sample(control.estimate, size = 1:length(control.estimate), replace = TRUE))
}
j <- (alpha/2) * B
k <- (1-(alpha/2)) * B
cat("95% CI for Hormone Group:", c(means.hormone[j],means.hormone[k]),"\n")
cat("95% CI for Control Group:", c(means.control[j],means.control[k]),"\n")

#part d: Permutation Tests
# Use permutation tests to show that there is no difference between the mean recurrence times
#Null hypothesis: the mean recurrence time between the two groups is equal
P <- 2000 #number of permutations
est <- c(control.estimate,hormone.estimate)
id <- c(rep(0,length(control.estimate)),rep(1,length(hormone.estimate)))
df <- data.frame(est,id)
theta.h <- mean(df[df$id == 1,1])
theta.c <- mean(df[df$id == 0,1])
res.diff.mean <- rep(0,P)
for(p in 1:P){
  perm <- sample(nrow(df))
  dat <- transform(df, id = id[perm])
  theta.h <- mean(dat[dat$id == 1,"est"])
  theta.c <- mean(dat[dat$id == 0,"est"])
  res.diff.mean[p] <- theta.h - theta.c
}
obs.diff.mean <- theta.h - theta.c
hist(res.diff.mean, breaks = 25, col = "gray", main = "Permutation Test for Difference of the Means")
abline(v = obs.diff.mean, col = "red")

quantile(res.diff.mean,probs = c(0.025,0.975))
obs.diff.mean

```

\newpage 
# Problem 5
```{r Problem 5}
set.seed(1000)
n <- 1000 #sample size
comp <- sample(1:2, prob = c(0.8,0.2), size = n, replace = TRUE)
mu = c(0,3)
stan.dev = sqrt(c(1,1))
samp <- rnorm(n, mean = mu[comp], sd = stan.dev[comp])

h1 <- 1.06*(n^(-1/5))*sd(samp)
h2 <- 0.9*(n^(-1/5))*min(sd(samp),(IQR(samp))/1.34)

plot(density(samp),col = 1,main = "Gaussian Kernel Density", lwd = 2) #true density estimate
lines(density(samp, bw = h1),col = 2)
lines(density(samp, bw =h2),col = 3)
legend("topright",c("True Density Mixture","h1 Bandwidth","h2 Bandwidth"),lty=1,col=1:3)

#part b
plot(density(samp, kernel = "epanechnikov"),col = 1,main = "Epanechnikov Kernel Density", lwd = 2) #true density estimate
lines(density(samp, bw = h1, kernel = "epanechnikov"),col = 2)
lines(density(samp, bw =h2, kernel = "epanechnikov"),col = 3)
legend("topright",c("True Density Mixture","h1 Bandwidth","h2 Bandwidth"),lty=1,col=1:3)

plot(density(samp, kernel = "rectangular"),col = 1,main = "Rectangular Kernel Density",lwd = 2) #true density estimate
lines(density(samp, bw = h1, kernel = "rectangular"),col = 2)
lines(density(samp, bw =h2, kernel = "rectangular"),col = 3)
legend("topright",c("True Density Mixture","h1 Bandwidth","h2 Bandwidth"),lty=1,col=1:3)

plot(density(samp, kernel = "triangular"),col = 1,main = "Triangular Kernel Density", lwd = 2) #true density estimate
lines(density(samp, bw = h1, kernel = "triangular"),col = 2)
lines(density(samp, bw =h2, kernel = "triangular"),col = 3)
legend("topright",c("True Density Mixture","h1 Bandwidth","h2 Bandwidth"),lty=1,col=1:3)

```
Based on the plots from above: The h2 bandwidth is a better smoothing parameter compared to using the h1 bandwidth because the h2 bandwidth plot fits over the true density mixture plots. Moreover, this shows that while changing the kernel type will change the shape of the mixture plot, the h2 bandwidth still fits over the true density mixture better than the h1 bandwidth plot. The h1 bandwidth plot is hardly affected when changing the kernel.
